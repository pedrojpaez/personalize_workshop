{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommendation on Amazon SageMaker with Factorization Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM1: Download ml-100k data  \n",
    "***The data sets are needed to train our Factorization Machine. We use the 100,000 movie ratings given by users from MovieLens data sets.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip -o ml-100k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Information\n",
    "*ua.base : data for training*  \n",
    "*ua.test : data for test/validation*  \n",
    "*Headers/columns :* ***user id | item id | rating (1-5) | timestamp***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM2: Let's shuffle rating items data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns in ua.base and ua.test file:\n",
    "user id | item id | rating | timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The code below will show how ua.test file look like for first 10 lines:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./ml-100k/ua.base', sep='\\t', names=['USER_ID', 'ITEM_ID', 'RATING', 'TIMESTAMP'])\n",
    "test_df = pd.read_csv('./ml-100k/ua.test', sep='\\t', names=['USER_ID', 'ITEM_ID', 'RATING', 'TIMESTAMP'])\n",
    "pd.set_option('display.max_rows', 5)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./ml-100k/u.user', sep='|', names=['user_id','age','gender','occupation','zip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./ml-100k/u.item', sep='|', names=['item_id','title','release_date','video_release_date','imdb_url','UNKOWN','Action','Adventure','Animation','Children','Comedy','Crime','Documentary','Drama','Fantasy','Noir','Horror','Musical','Mystery','Romance','SciFi','Thriller','War','Western'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM3: Build training set and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Import necessary modules***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, csv, io, json\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import json_deserializer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set S3 bucket and prefix***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'factorization-machine-sagemaker'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Initialize number of total users and movies in data set, as well as number of train and test data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbUsers=943\n",
    "nbMovies=1682\n",
    "nbFeatures=nbUsers+nbMovies\n",
    "\n",
    "nbRatingsTrain=90570\n",
    "nbRatingsTest=9430"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***For each user, build a list of rated movies with their ratings. We'd need this to add random negative samples.***  \n",
    "This is achieved by a dictionary moviesByUser that will look like this:  \n",
    "```\n",
    "{\n",
    "  '0':[[875072546,4],[875072441,3]],\n",
    "  '1':[[887431882,2]]\n",
    "}\n",
    "```\n",
    "where key represents userId (stored as userId - 1) and each element in the values represents movieId (stored as movieId -1) as first element and the rating as second element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesByUser = {}\n",
    "for userId in range(nbUsers):\n",
    "    moviesByUser[str(userId)]=[]\n",
    " \n",
    "with open('./ml-100k/ua.base','r') as f:\n",
    "    samples=csv.reader(f,delimiter='\\t')\n",
    "    for userId,movieId,rating,timestamp in samples:\n",
    "        moviesByUser[str(int(userId)-1)].append([int(movieId)-1,rating]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM4: Define method to load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The data will be loaded into 2 vectors: feature vector X and label vector Y***  \n",
    "Feature vector X is a one-hot encoded vector that sticks and flattens user Ids and movie Ids together. It should look like this below (without rows' and columns' labels):   \n",
    "\n",
    "|<pre></pre>| 1 \t    |      2    |    3  \t|<pre>...</pre>| 1  | 2  |<pre>...</pre>|\n",
    "|   :---:   |:---:      |    :---:\t|   :---:\t|  :---:\t|  :---:\t |  :---:     |   :---:\t  |\n",
    "| **data0** | 1 \t    |<pre></pre>|<pre></pre>|<pre></pre>|<pre></pre> |<pre>1</pre>|<pre></pre>|\n",
    "| **data1** |<pre></pre>| 1 \t    |<pre></pre>|<pre></pre>|<pre>1</pre>|<pre></pre> |<pre></pre>|\n",
    "|<pre>...</pre>|<pre></pre>|<pre></pre>|<pre></pre>|<pre></pre>|<pre></pre> |<pre></pre> |<pre></pre>|\n",
    " \n",
    "It is a 2D sparse matrix where columns are user Ids and movie Ids, and rows are data items in the training/test data set.\n",
    "One row represents 1 training/test data that has 2 ones (1s) that mark the user Id and movie Id that he/she rated.   \n",
    "\n",
    "Label vector Y is a 1D vector containing expected output. It looks like this below (without rows' labels):\n",
    "\n",
    "|<pre></pre>|<pre></pre>|\n",
    "| :--- | :---:|\n",
    "|**data0**| 1 |\n",
    "|**data1**| 1 |\n",
    "|**data2**| 0 |\n",
    "|**data3**| 1 |\n",
    "|<pre>...</pre>|<pre></pre>|\n",
    "\n",
    "\n",
    "If user's rating for that movie is 4 or 5, then value is 1, otherwise 0. Each element corresponds to one data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(filename, lines, columns):\n",
    "\n",
    "    # Features are one-hot encoded in a sparse matrix\n",
    "    X = lil_matrix((lines, columns)).astype('float32')\n",
    "    # Labels are stored in a vector\n",
    "    Y = []\n",
    "    line=0\n",
    "    with open(filename,'r') as f:\n",
    "        samples=csv.reader(f,delimiter='\\t')\n",
    "        for userId,movieId,rating,timestamp in samples:\n",
    "            X[line,int(userId)-1] = 1\n",
    "            X[line,int(nbUsers)+int(movieId)-1] = 1\n",
    "            if int(rating) >= 4:\n",
    "                Y.append(1)\n",
    "            else:\n",
    "                Y.append(0)\n",
    "            line=line+1\n",
    "            \n",
    "    Y=np.array(Y).astype('float32')\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now that we have defined the loadDataset method, lets load both training and test data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = loadDataset('./ml-100k/ua.base', nbRatingsTrain, nbFeatures)\n",
    "X_test, Y_test = loadDataset('./ml-100k/ua.test',nbRatingsTest,nbFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Let's examine the dimensions of X and Y vectors***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "assert X_train.shape == (nbRatingsTrain, nbFeatures)\n",
    "assert Y_train.shape == (nbRatingsTrain, )\n",
    "zero_labels = np.count_nonzero(Y_train)\n",
    "print(\"Training labels: %d zeros, %d ones\" % (zero_labels, nbRatingsTrain-zero_labels))\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "assert X_test.shape  == (nbRatingsTest, nbFeatures)\n",
    "assert Y_test.shape  == (nbRatingsTest, )\n",
    "zero_labels = np.count_nonzero(Y_test)\n",
    "print(\"Test labels: %d zeros, %d ones\" % (zero_labels, nbRatingsTest-zero_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM5: Convert to protobuf and save to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_key      = 'train.protobuf'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train3')\n",
    "\n",
    "test_key       = 'test.protobuf'\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test3')\n",
    "\n",
    "output_prefix  = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeDatasetToProtobuf(X, Y, bucket, prefix, key):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, Y)\n",
    "    buf.seek(0)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)\n",
    "    \n",
    "train_data = writeDatasetToProtobuf(X_train, Y_train, bucket, train_prefix, train_key)    \n",
    "test_data  = writeDatasetToProtobuf(X_test, Y_test, bucket, test_prefix, test_key)    \n",
    "  \n",
    "print(train_data)\n",
    "print(test_data)\n",
    "print('Output: {}'.format(output_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM6: Run training job\n",
    "***We are done with the data preparation part. Let's begin training our Factorization Machine model.***  \n",
    "***SageMaker provides both the container and built-in algorithm to run the training and inference.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the list of container images containing built-in algorithm for factorization machine in SageMaker per region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/factorization-machines:latest',\n",
    "              'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/factorization-machines:latest',\n",
    "              'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/factorization-machines:latest',\n",
    "              'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/factorization-machines:latest',\n",
    "              'ap-southeast-1': '475088953585.dkr.ecr.ap-southeast-1.amazonaws.com/factorization-machines:latest'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Behing the scene, SageMaker provisions a container to run the training, and terminate it after training job succeeds. Metrics during training, including accuracy are posted to CloudWatch Metrics.***    \n",
    "\n",
    "Note: If you like GUI (Graphical User Interface), you can execute the training via AWS Console too. Basically we can interact with AWS in 3 ways: AWS Console (GUI), CLI, and SDK. For this lab, we are using SDK. You can inspect https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs (change the region as necessary) to see the running training job after you run the step below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n",
    "                                   get_execution_role(), \n",
    "                                   train_instance_count=1, \n",
    "                                   train_instance_type='ml.c4.xlarge',\n",
    "                                   output_path=output_prefix,\n",
    "                                   sagemaker_session=sagemaker.Session())\n",
    "\n",
    "fm.set_hyperparameters(feature_dim=nbFeatures,\n",
    "                      predictor_type='binary_classifier',\n",
    "                      mini_batch_size=1000,\n",
    "                      num_factors=64,\n",
    "                      epochs=10)\n",
    "\n",
    "fm.fit({'train': train_data, 'test': test_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***If the training was successful, you wil see 'Training job completed' at the end of the output. Scroll up to see the  train and test accuracy***    \n",
    "\n",
    "***After training phase completed, we have the model parameters stored in S3 (in the output path you specified). You can check your S3 bucket that contains the output to inspect how the training job output looks like***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM7: Deploy model\n",
    "\n",
    "***Now, let's deploy the model for inference using SageMaker SDK. It will spin-up a new virtual machine with container containing algorithm for inference. It will give us an API endpoint for inference.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor = fm.deploy(instance_type='ml.t2.medium', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM8: Run predictions\n",
    "\n",
    "***After the model is deployed and given an endpoint, we can run the prediction / inference.***  \n",
    "Below we define the serializer and deserializer for the prediction request/response data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fm_serializer(data):\n",
    "    js = {'instances': []}\n",
    "    for row in data:\n",
    "        js['instances'].append({'features': row.tolist()})\n",
    "    #print js\n",
    "    return json.dumps(js)\n",
    "\n",
    "fm_predictor.content_type = 'application/json'\n",
    "fm_predictor.serializer = fm_serializer\n",
    "fm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Let's test the prediction with some data from the test set***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_from = 900\n",
    "index_to = 910\n",
    "result = fm_predictor.predict(X_test[index_from:index_to].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Display the prediction in pretty table, being compared againts the actual rating (label) from the test set.***.      \n",
    "Observe that for score between 0.3 to 0.7 our recommender may guess incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabulate\n",
    "import tabulate\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "scores, predicted_rating = ['Score'], ['Predicted Rating']\n",
    "for r in result['predictions']:\n",
    "    scores.append(\"%.2f\" % r['score'])\n",
    "    predicted_rating.append(r['predicted_label'])\n",
    "\n",
    "\n",
    "table = [scores, predicted_rating, ['Actual Rating'] + Y_test[index_from:index_to].tolist() ]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM9: Get Movies Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***After testing the prediction, let's get real movies recommendation for a particular user***    \n",
    "First, let's prepare a dictionary that maps movie ID to its title. We use the u.item data containing movies' details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = {}\n",
    "with open('./ml-100k/u.item','r') as f:\n",
    "    samples=csv.reader(f,delimiter='|')\n",
    "    for movieId,m_title,r_date,video_r_date,imdb_URL,unkwn,act,adv,anm,kid,cmd,crime,doc,drama,fantasy,f_noir,horror,msc,myst,rom,sfy,thriller,war,west in samples:\n",
    "        movies[int(movieId)] = m_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Define some parameters***    \n",
    "userId = the ID of user who needs the recommendations    \n",
    "score_threshold = Cut-off score. Value nearer to 1 means that we only consider strong predictions. Value 0.5 is the minimun.    \n",
    "maximum_recommendations = Maximum of movies recommendation. The actual result may be less than this if not many movies are strongly recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userId = 344\n",
    "score_threshold = 0.50\n",
    "maximum_recommendations = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Run predictions for all movies for this particular user and sort the output based on score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_movies=[]\n",
    "for movieId in range(nbMovies):\n",
    "    test_input = lil_matrix((1, nbFeatures)).astype('float32')\n",
    "    test_input[0, int(userId)-1] = 1\n",
    "    test_input[0, nbUsers+int(movieId)-1] = 1\n",
    "    result = fm_predictor.predict(test_input.toarray())\n",
    "    result_label, result_score = int(result['predictions'][0]['predicted_label']), float(result['predictions'][0]['score'])\n",
    "    if (result_label == 1) and (result_score > score_threshold):\n",
    "        recommended_movies.append([int(movieId),result_score])\n",
    "        \n",
    "def getVal(item):\n",
    "    return item[1]\n",
    "recommended_movies = sorted(recommended_movies,key=getVal,reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Print out the result of top recommended movies***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_table = [['<strong>Movie Title</strong>','<strong>Score</strong>']]\n",
    "for i in range(min(maximum_recommendations,len(recommended_movies))):\n",
    "    output_table.append([movies[int(recommended_movies[i][0])],recommended_movies[i][1]])\n",
    "\n",
    "display(HTML(tabulate.tabulate(output_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Compare the recommendation with the top 20 movies that are actually rated by that particular user, sorted from the highest rating***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_rated_movies(user_id, k):\n",
    "    rated_movies = moviesByUser[str(int(user_id)-1)]\n",
    "    rated_movies = sorted(rated_movies,key=getVal,reverse=True)\n",
    "    results = []\n",
    "    \n",
    "    for movie in rated_movies:\n",
    "        results.append([movies[int(movie[0]+1)],movie[1]])\n",
    "    return results[0:k]\n",
    "\n",
    "output_table = [['<strong>Movie Title</strong>','<strong>Actual Rating</strong>']]\n",
    "for m in find_top_rated_movies(userId,20):\n",
    "    output_table.append(m)\n",
    "\n",
    "display(HTML(tabulate.tabulate(output_table, tablefmt='html')))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python2",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
