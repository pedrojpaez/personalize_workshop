{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommendation on Amazon SageMaker with Factorization Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM1: Download ml-100k data  \n",
    "***The data sets are needed to train our Factorization Machine. We use the 100,000 movie ratings given by users from MovieLens data sets.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in /home/ec2-user/anaconda3/envs/python2/lib/python2.7/site-packages (1.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/ec2-user/anaconda3/envs/python2/lib/python2.7/site-packages (from pydot) (2.2.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-29 17:29:52--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4924029 (4.7M) [application/zip]\n",
      "Saving to: ‘ml-100k.zip.1’\n",
      "\n",
      "ml-100k.zip.1       100%[===================>]   4.70M  2.25MB/s    in 2.1s    \n",
      "\n",
      "2019-10-29 17:29:55 (2.25 MB/s) - ‘ml-100k.zip.1’ saved [4924029/4924029]\n",
      "\n",
      "Archive:  ml-100k.zip\n",
      "   creating: ml-100k/\n",
      "  inflating: ml-100k/allbut.pl       \n",
      "  inflating: ml-100k/mku.sh          \n",
      "  inflating: ml-100k/README          \n",
      "  inflating: ml-100k/u.data          \n",
      "  inflating: ml-100k/u.genre         \n",
      "  inflating: ml-100k/u.info          \n",
      "  inflating: ml-100k/u.item          \n",
      "  inflating: ml-100k/u.occupation    \n",
      "  inflating: ml-100k/u.user          \n",
      "  inflating: ml-100k/u1.base         \n",
      "  inflating: ml-100k/u1.test         \n",
      "  inflating: ml-100k/u2.base         \n",
      "  inflating: ml-100k/u2.test         \n",
      "  inflating: ml-100k/u3.base         \n",
      "  inflating: ml-100k/u3.test         \n",
      "  inflating: ml-100k/u4.base         \n",
      "  inflating: ml-100k/u4.test         \n",
      "  inflating: ml-100k/u5.base         \n",
      "  inflating: ml-100k/u5.test         \n",
      "  inflating: ml-100k/ua.base         \n",
      "  inflating: ml-100k/ua.test         \n",
      "  inflating: ml-100k/ub.base         \n",
      "  inflating: ml-100k/ub.test         \n"
     ]
    }
   ],
   "source": [
    "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip -o ml-100k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Information\n",
    "*ua.base : data for training*  \n",
    "*ua.test : data for test/validation*  \n",
    "*Headers/columns :* ***user id | item id | rating (1-5) | timestamp***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM2: Let's shuffle rating items data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns in ua.base and ua.test file:\n",
    "user id | item id | rating | timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The code below will show how ua.test file look like for first 10 lines:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>RATING</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>874965758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>876893171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90568</th>\n",
       "      <td>943</td>\n",
       "      <td>1228</td>\n",
       "      <td>3</td>\n",
       "      <td>888640275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90569</th>\n",
       "      <td>943</td>\n",
       "      <td>1330</td>\n",
       "      <td>3</td>\n",
       "      <td>888692465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90570 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       USER_ID  ITEM_ID  RATING  TIMESTAMP\n",
       "0            1        1       5  874965758\n",
       "1            1        2       3  876893171\n",
       "...        ...      ...     ...        ...\n",
       "90568      943     1228       3  888640275\n",
       "90569      943     1330       3  888692465\n",
       "\n",
       "[90570 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./ml-100k/ua.base', sep='\\t', names=['USER_ID', 'ITEM_ID', 'RATING', 'TIMESTAMP'])\n",
    "test_df = pd.read_csv('./ml-100k/ua.test', sep='\\t', names=['USER_ID', 'ITEM_ID', 'RATING', 'TIMESTAMP'])\n",
    "pd.set_option('display.max_rows', 5)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>85711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>94043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>942</td>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>librarian</td>\n",
       "      <td>78209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>943</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>student</td>\n",
       "      <td>77841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>943 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  age gender  occupation    zip\n",
       "0          1   24      M  technician  85711\n",
       "1          2   53      F       other  94043\n",
       "..       ...  ...    ...         ...    ...\n",
       "941      942   48      F   librarian  78209\n",
       "942      943   22      M     student  77841\n",
       "\n",
       "[943 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./ml-100k/u.user', sep='|', names=['user_id','age','gender','occupation','zip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>video_release_date</th>\n",
       "      <th>imdb_url</th>\n",
       "      <th>UNKOWN</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children</th>\n",
       "      <th>...</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Noir</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>SciFi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Toy%20Story%2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?GoldenEye%20(...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>1681</td>\n",
       "      <td>You So Crazy (1994)</td>\n",
       "      <td>01-Jan-1994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?You%20So%20Cr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>1682</td>\n",
       "      <td>Scream of Stone (Schrei aus Stein) (1991)</td>\n",
       "      <td>08-Mar-1996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Schrei%20aus%...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1682 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id                                      title release_date  \\\n",
       "0           1                           Toy Story (1995)  01-Jan-1995   \n",
       "1           2                           GoldenEye (1995)  01-Jan-1995   \n",
       "...       ...                                        ...          ...   \n",
       "1680     1681                        You So Crazy (1994)  01-Jan-1994   \n",
       "1681     1682  Scream of Stone (Schrei aus Stein) (1991)  08-Mar-1996   \n",
       "\n",
       "      video_release_date                                           imdb_url  \\\n",
       "0                    NaN  http://us.imdb.com/M/title-exact?Toy%20Story%2...   \n",
       "1                    NaN  http://us.imdb.com/M/title-exact?GoldenEye%20(...   \n",
       "...                  ...                                                ...   \n",
       "1680                 NaN  http://us.imdb.com/M/title-exact?You%20So%20Cr...   \n",
       "1681                 NaN  http://us.imdb.com/M/title-exact?Schrei%20aus%...   \n",
       "\n",
       "      UNKOWN  Action  Adventure  Animation  Children  ...  Fantasy  Noir  \\\n",
       "0          0       0          0          1         1  ...        0     0   \n",
       "1          0       1          1          0         0  ...        0     0   \n",
       "...      ...     ...        ...        ...       ...  ...      ...   ...   \n",
       "1680       0       0          0          0         0  ...        0     0   \n",
       "1681       0       0          0          0         0  ...        0     0   \n",
       "\n",
       "      Horror  Musical  Mystery  Romance  SciFi  Thriller  War  Western  \n",
       "0          0        0        0        0      0         0    0        0  \n",
       "1          0        0        0        0      0         1    0        0  \n",
       "...      ...      ...      ...      ...    ...       ...  ...      ...  \n",
       "1680       0        0        0        0      0         0    0        0  \n",
       "1681       0        0        0        0      0         0    0        0  \n",
       "\n",
       "[1682 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./ml-100k/u.item', sep='|', names=['item_id','title','release_date','video_release_date','imdb_url','UNKOWN','Action','Adventure','Animation','Children','Comedy','Crime','Documentary','Drama','Fantasy','Noir','Horror','Musical','Mystery','Romance','SciFi','Thriller','War','Western'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM3: Build training set and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Import necessary modules***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, csv, io, json\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import json_deserializer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set S3 bucket and prefix***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::349934754982:role/service-role/AmazonSageMaker-ExecutionRole-20190918T150782\n",
      "CPU times: user 355 ms, sys: 57 ms, total: 412 ms\n",
      "Wall time: 4.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'factorization-machine-sagemaker'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Initialize number of total users and movies in data set, as well as number of train and test data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbUsers=943\n",
    "nbMovies=1682\n",
    "nbFeatures=nbUsers+nbMovies\n",
    "\n",
    "nbRatingsTrain=90570\n",
    "nbRatingsTest=9430"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***For each user, build a list of rated movies with their ratings. We'd need this to add random negative samples.***  \n",
    "This is achieved by a dictionary moviesByUser that will look like this:  \n",
    "```\n",
    "{\n",
    "  '0':[[875072546,4],[875072441,3]],\n",
    "  '1':[[887431882,2]]\n",
    "}\n",
    "```\n",
    "where key represents userId (stored as userId - 1) and each element in the values represents movieId (stored as movieId -1) as first element and the rating as second element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesByUser = {}\n",
    "for userId in range(nbUsers):\n",
    "    moviesByUser[str(userId)]=[]\n",
    " \n",
    "with open('./ml-100k/ua.base','r') as f:\n",
    "    samples=csv.reader(f,delimiter='\\t')\n",
    "    for userId,movieId,rating,timestamp in samples:\n",
    "        moviesByUser[str(int(userId)-1)].append([int(movieId)-1,rating]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM4: Define method to load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The data will be loaded into 2 vectors: feature vector X and label vector Y***  \n",
    "Feature vector X is a one-hot encoded vector that sticks and flattens user Ids and movie Ids together. It should look like this below (without rows' and columns' labels):   \n",
    "\n",
    "|<pre></pre>| 1 \t    |      2    |    3  \t|<pre>...</pre>| 1  | 2  |<pre>...</pre>|\n",
    "|   :---:   |:---:      |    :---:\t|   :---:\t|  :---:\t|  :---:\t |  :---:     |   :---:\t  |\n",
    "| **data0** | 1 \t    |<pre></pre>|<pre></pre>|<pre></pre>|<pre></pre> |<pre>1</pre>|<pre></pre>|\n",
    "| **data1** |<pre></pre>| 1 \t    |<pre></pre>|<pre></pre>|<pre>1</pre>|<pre></pre> |<pre></pre>|\n",
    "|<pre>...</pre>|<pre></pre>|<pre></pre>|<pre></pre>|<pre></pre>|<pre></pre> |<pre></pre> |<pre></pre>|\n",
    " \n",
    "It is a 2D sparse matrix where columns are user Ids and movie Ids, and rows are data items in the training/test data set.\n",
    "One row represents 1 training/test data that has 2 ones (1s) that mark the user Id and movie Id that he/she rated.   \n",
    "\n",
    "Label vector Y is a 1D vector containing expected output. It looks like this below (without rows' labels):\n",
    "\n",
    "|<pre></pre>|<pre></pre>|\n",
    "| :--- | :---:|\n",
    "|**data0**| 1 |\n",
    "|**data1**| 1 |\n",
    "|**data2**| 0 |\n",
    "|**data3**| 1 |\n",
    "|<pre>...</pre>|<pre></pre>|\n",
    "\n",
    "\n",
    "If user's rating for that movie is 4 or 5, then value is 1, otherwise 0. Each element corresponds to one data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(filename, lines, columns):\n",
    "\n",
    "    # Features are one-hot encoded in a sparse matrix\n",
    "    X = lil_matrix((lines, columns)).astype('float32')\n",
    "    # Labels are stored in a vector\n",
    "    Y = []\n",
    "    line=0\n",
    "    with open(filename,'r') as f:\n",
    "        samples=csv.reader(f,delimiter='\\t')\n",
    "        for userId,movieId,rating,timestamp in samples:\n",
    "            X[line,int(userId)-1] = 1\n",
    "            X[line,int(nbUsers)+int(movieId)-1] = 1\n",
    "            if int(rating) >= 4:\n",
    "                Y.append(1)\n",
    "            else:\n",
    "                Y.append(0)\n",
    "            line=line+1\n",
    "            \n",
    "    Y=np.array(Y).astype('float32')\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now that we have defined the loadDataset method, lets load both training and test data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = loadDataset('./ml-100k/ua.base', nbRatingsTrain, nbFeatures)\n",
    "X_test, Y_test = loadDataset('./ml-100k/ua.test',nbRatingsTest,nbFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Let's examine the dimensions of X and Y vectors***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90570, 2625)\n",
      "(90570,)\n",
      "Training labels: 49906 zeros, 40664 ones\n",
      "(9430, 2625)\n",
      "(9430,)\n",
      "Test labels: 5469 zeros, 3961 ones\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "assert X_train.shape == (nbRatingsTrain, nbFeatures)\n",
    "assert Y_train.shape == (nbRatingsTrain, )\n",
    "zero_labels = np.count_nonzero(Y_train)\n",
    "print(\"Training labels: %d zeros, %d ones\" % (zero_labels, nbRatingsTrain-zero_labels))\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "assert X_test.shape  == (nbRatingsTest, nbFeatures)\n",
    "assert Y_test.shape  == (nbRatingsTest, )\n",
    "zero_labels = np.count_nonzero(Y_test)\n",
    "print(\"Test labels: %d zeros, %d ones\" % (zero_labels, nbRatingsTest-zero_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM5: Convert to protobuf and save to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_key      = 'train.protobuf'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train3')\n",
    "\n",
    "test_key       = 'test.protobuf'\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test3')\n",
    "\n",
    "output_prefix  = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-southeast-1-349934754982/factorization-machine-sagemaker/train3/train.protobuf\n",
      "s3://sagemaker-ap-southeast-1-349934754982/factorization-machine-sagemaker/test3/test.protobuf\n",
      "Output: s3://sagemaker-ap-southeast-1-349934754982/factorization-machine-sagemaker/output\n"
     ]
    }
   ],
   "source": [
    "def writeDatasetToProtobuf(X, Y, bucket, prefix, key):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, Y)\n",
    "    buf.seek(0)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)\n",
    "    \n",
    "train_data = writeDatasetToProtobuf(X_train, Y_train, bucket, train_prefix, train_key)    \n",
    "test_data  = writeDatasetToProtobuf(X_test, Y_test, bucket, test_prefix, test_key)    \n",
    "  \n",
    "print(train_data)\n",
    "print(test_data)\n",
    "print('Output: {}'.format(output_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM6: Run training job\n",
    "***We are done with the data preparation part. Let's begin training our Factorization Machine model.***  \n",
    "***SageMaker provides both the container and built-in algorithm to run the training and inference.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the list of container images containing built-in algorithm for factorization machine in SageMaker per region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/factorization-machines:latest',\n",
    "              'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/factorization-machines:latest',\n",
    "              'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/factorization-machines:latest',\n",
    "              'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/factorization-machines:latest',\n",
    "              'ap-southeast-1': '475088953585.dkr.ecr.ap-southeast-1.amazonaws.com/factorization-machines:latest'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Behing the scene, SageMaker provisions a container to run the training, and terminate it after training job succeeds. Metrics during training, including accuracy are posted to CloudWatch Metrics.***    \n",
    "\n",
    "Note: If you like GUI (Graphical User Interface), you can execute the training via AWS Console too. Basically we can interact with AWS in 3 ways: AWS Console (GUI), CLI, and SDK. For this lab, we are using SDK. You can inspect https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs (change the region as necessary) to see the running training job after you run the step below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-29 17:31:18 Starting - Starting the training job...\n",
      "2019-10-29 17:31:23 Starting - Launching requested ML instances......\n",
      "2019-10-29 17:32:22 Starting - Preparing the instances for training...\n",
      "2019-10-29 17:33:14 Downloading - Downloading input data...\n",
      "2019-10-29 17:33:42 Training - Downloading the training image..\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': 1, u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'epochs': u'10', u'feature_dim': u'2625', u'mini_batch_size': u'1000', u'predictor_type': u'binary_classifier', u'num_factors': u'64'}\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] Final configuration: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': u'10', u'feature_dim': u'2625', u'num_factors': u'64', u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'predictor_type': u'binary_classifier', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 WARNING 139641366697792] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] Using default worker.\u001b[0m\n",
      "\u001b[31m[2019-10-29 17:34:01.127] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[31m[2019-10-29 17:34:01.132] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 8, \"num_examples\": 1, \"num_bytes\": 64000}\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] nvidia-smi took: 0.0252130031586 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 38.11907768249512, \"sum\": 38.11907768249512, \"min\": 38.11907768249512}}, \"EndTime\": 1572370441.168792, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572370441.12294}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1572370441.168977, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572370441.168942}\n",
      "\u001b[0m\n",
      "\u001b[31m[17:34:01] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.202139.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[31m[17:34:01] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.202139.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] #quality_metric: host=algo-1, epoch=0, batch=0 train binary_classification_accuracy <score>=0.589\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] #quality_metric: host=algo-1, epoch=0, batch=0 train binary_classification_cross_entropy <loss>=0.691272155762\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] #quality_metric: host=algo-1, epoch=0, batch=0 train binary_f_1.000 <score>=0.722859069454\u001b[0m\n",
      "\u001b[31m[2019-10-29 17:34:01.776] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 581, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] #quality_metric: host=algo-1, epoch=0, train binary_classification_accuracy <score>=0.560692307692\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] #quality_metric: host=algo-1, epoch=0, train binary_classification_cross_entropy <loss>=0.689987980099\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] #quality_metric: host=algo-1, epoch=0, train binary_f_1.000 <score>=0.700144763391\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"update.time\": {\"count\": 1, \"max\": 607.7229976654053, \"sum\": 607.7229976654053, \"min\": 607.7229976654053}}, \"EndTime\": 1572370441.776935, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572370441.168872}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Total Batches Seen\": {\"count\": 1, \"max\": 92, \"sum\": 92.0, \"min\": 92}, \"Total Records Seen\": {\"count\": 1, \"max\": 91570, \"sum\": 91570.0, \"min\": 91570}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1572370441.777152, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 0}, \"StartTime\": 1572370441.169183}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] #throughput_metric: host=algo-1, train throughput=148945.407826 records/second\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] #quality_metric: host=algo-1, epoch=1, batch=0 train binary_classification_accuracy <score>=0.587\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] #quality_metric: host=algo-1, epoch=1, batch=0 train binary_classification_cross_entropy <loss>=0.671663574219\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:01 INFO 139641366697792] #quality_metric: host=algo-1, epoch=1, batch=0 train binary_f_1.000 <score>=0.739760554505\u001b[0m\n",
      "\u001b[31m[2019-10-29 17:34:02.477] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 698, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:02 INFO 139641366697792] #quality_metric: host=algo-1, epoch=1, train binary_classification_accuracy <score>=0.566318681319\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:02 INFO 139641366697792] #quality_metric: host=algo-1, epoch=1, train binary_classification_cross_entropy <loss>=0.683364412077\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:02 INFO 139641366697792] #quality_metric: host=algo-1, epoch=1, train binary_f_1.000 <score>=0.704795529857\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 700.6099224090576, \"sum\": 700.6099224090576, \"min\": 700.6099224090576}}, \"EndTime\": 1572370442.478495, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572370441.777016}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:02 INFO 139641366697792] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Total Batches Seen\": {\"count\": 1, \"max\": 183, \"sum\": 183.0, \"min\": 183}, \"Total Records Seen\": {\"count\": 1, \"max\": 182140, \"sum\": 182140.0, \"min\": 182140}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1572370442.478927, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 1}, \"StartTime\": 1572370441.777824}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:02 INFO 139641366697792] #throughput_metric: host=algo-1, train throughput=129146.240547 records/second\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:02 INFO 139641366697792] #quality_metric: host=algo-1, epoch=2, batch=0 train binary_classification_accuracy <score>=0.587\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:02 INFO 139641366697792] #quality_metric: host=algo-1, epoch=2, batch=0 train binary_classification_cross_entropy <loss>=0.665132446289\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:02 INFO 139641366697792] #quality_metric: host=algo-1, epoch=2, batch=0 train binary_f_1.000 <score>=0.739760554505\u001b[0m\n",
      "\u001b[31m[2019-10-29 17:34:03.176] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 694, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #quality_metric: host=algo-1, epoch=2, train binary_classification_accuracy <score>=0.576098901099\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #quality_metric: host=algo-1, epoch=2, train binary_classification_cross_entropy <loss>=0.678237371759\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #quality_metric: host=algo-1, epoch=2, train binary_f_1.000 <score>=0.707536941705\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 696.8939304351807, \"sum\": 696.8939304351807, \"min\": 696.8939304351807}}, \"EndTime\": 1572370443.176626, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572370442.478586}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Total Batches Seen\": {\"count\": 1, \"max\": 274, \"sum\": 274.0, \"min\": 274}, \"Total Records Seen\": {\"count\": 1, \"max\": 272710, \"sum\": 272710.0, \"min\": 272710}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1572370443.176878, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 2}, \"StartTime\": 1572370442.4797}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #throughput_metric: host=algo-1, train throughput=129884.493339 records/second\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #quality_metric: host=algo-1, epoch=3, batch=0 train binary_classification_accuracy <score>=0.587\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #quality_metric: host=algo-1, epoch=3, batch=0 train binary_classification_cross_entropy <loss>=0.659431640625\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #quality_metric: host=algo-1, epoch=3, batch=0 train binary_f_1.000 <score>=0.739760554505\u001b[0m\n",
      "\u001b[31m[2019-10-29 17:34:03.872] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 692, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #quality_metric: host=algo-1, epoch=3, train binary_classification_accuracy <score>=0.586\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #quality_metric: host=algo-1, epoch=3, train binary_classification_cross_entropy <loss>=0.673689472576\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #quality_metric: host=algo-1, epoch=3, train binary_f_1.000 <score>=0.709914377233\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 694.9520111083984, \"sum\": 694.9520111083984, \"min\": 694.9520111083984}}, \"EndTime\": 1572370443.872566, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572370443.176708}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Total Batches Seen\": {\"count\": 1, \"max\": 365, \"sum\": 365.0, \"min\": 365}, \"Total Records Seen\": {\"count\": 1, \"max\": 363280, \"sum\": 363280.0, \"min\": 363280}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1572370443.872796, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 3}, \"StartTime\": 1572370443.177584}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #throughput_metric: host=algo-1, train throughput=130247.392855 records/second\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #quality_metric: host=algo-1, epoch=4, batch=0 train binary_classification_accuracy <score>=0.587\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #quality_metric: host=algo-1, epoch=4, batch=0 train binary_classification_cross_entropy <loss>=0.654440124512\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:03 INFO 139641366697792] #quality_metric: host=algo-1, epoch=4, batch=0 train binary_f_1.000 <score>=0.739760554505\u001b[0m\n",
      "\u001b[31m[2019-10-29 17:34:04.434] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 559, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #quality_metric: host=algo-1, epoch=4, train binary_classification_accuracy <score>=0.594538461538\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #quality_metric: host=algo-1, epoch=4, train binary_classification_cross_entropy <loss>=0.669661624447\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #quality_metric: host=algo-1, epoch=4, train binary_f_1.000 <score>=0.712054877906\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 561.3110065460205, \"sum\": 561.3110065460205, \"min\": 561.3110065460205}}, \"EndTime\": 1572370444.434789, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572370443.872648}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Total Batches Seen\": {\"count\": 1, \"max\": 456, \"sum\": 456.0, \"min\": 456}, \"Total Records Seen\": {\"count\": 1, \"max\": 453850, \"sum\": 453850.0, \"min\": 453850}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1572370444.435054, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 4}, \"StartTime\": 1572370443.873444}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #throughput_metric: host=algo-1, train throughput=161230.307476 records/second\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #quality_metric: host=algo-1, epoch=5, batch=0 train binary_classification_accuracy <score>=0.587\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #quality_metric: host=algo-1, epoch=5, batch=0 train binary_classification_cross_entropy <loss>=0.650061401367\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #quality_metric: host=algo-1, epoch=5, batch=0 train binary_f_1.000 <score>=0.739102969046\u001b[0m\n",
      "\u001b[31m[2019-10-29 17:34:04.958] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 521, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #quality_metric: host=algo-1, epoch=5, train binary_classification_accuracy <score>=0.601516483516\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #quality_metric: host=algo-1, epoch=5, train binary_classification_cross_entropy <loss>=0.666080986274\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #quality_metric: host=algo-1, epoch=5, train binary_f_1.000 <score>=0.713719545892\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 523.3550071716309, \"sum\": 523.3550071716309, \"min\": 523.3550071716309}}, \"EndTime\": 1572370444.959054, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572370444.43488}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Total Batches Seen\": {\"count\": 1, \"max\": 547, \"sum\": 547.0, \"min\": 547}, \"Total Records Seen\": {\"count\": 1, \"max\": 544420, \"sum\": 544420.0, \"min\": 544420}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1572370444.959262, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 5}, \"StartTime\": 1572370444.435672}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #throughput_metric: host=algo-1, train throughput=172944.178142 records/second\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #quality_metric: host=algo-1, epoch=6, batch=0 train binary_classification_accuracy <score>=0.603\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #quality_metric: host=algo-1, epoch=6, batch=0 train binary_classification_cross_entropy <loss>=0.646207275391\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:04 INFO 139641366697792] #quality_metric: host=algo-1, epoch=6, batch=0 train binary_f_1.000 <score>=0.746325878594\u001b[0m\n",
      "\u001b[31m[2019-10-29 17:34:05.491] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 529, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:05 INFO 139641366697792] #quality_metric: host=algo-1, epoch=6, train binary_classification_accuracy <score>=0.60889010989\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:05 INFO 139641366697792] #quality_metric: host=algo-1, epoch=6, train binary_classification_cross_entropy <loss>=0.662880099454\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:05 INFO 139641366697792] #quality_metric: host=algo-1, epoch=6, train binary_f_1.000 <score>=0.716051155629\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 532.0048332214355, \"sum\": 532.0048332214355, \"min\": 532.0048332214355}}, \"EndTime\": 1572370445.491906, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572370444.959125}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:05 INFO 139641366697792] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Total Batches Seen\": {\"count\": 1, \"max\": 638, \"sum\": 638.0, \"min\": 638}, \"Total Records Seen\": {\"count\": 1, \"max\": 634990, \"sum\": 634990.0, \"min\": 634990}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1572370445.492169, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 6}, \"StartTime\": 1572370444.959868}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:05 INFO 139641366697792] #throughput_metric: host=algo-1, train throughput=170108.717905 records/second\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:05 INFO 139641366697792] #quality_metric: host=algo-1, epoch=7, batch=0 train binary_classification_accuracy <score>=0.616\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:05 INFO 139641366697792] #quality_metric: host=algo-1, epoch=7, batch=0 train binary_classification_cross_entropy <loss>=0.642798950195\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:05 INFO 139641366697792] #quality_metric: host=algo-1, epoch=7, batch=0 train binary_f_1.000 <score>=0.751937984496\u001b[0m\n",
      "\u001b[31m[2019-10-29 17:34:06.011] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 517, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #quality_metric: host=algo-1, epoch=7, train binary_classification_accuracy <score>=0.615461538462\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #quality_metric: host=algo-1, epoch=7, train binary_classification_cross_entropy <loss>=0.659999435928\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #quality_metric: host=algo-1, epoch=7, train binary_f_1.000 <score>=0.718032601952\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 519.0520286560059, \"sum\": 519.0520286560059, \"min\": 519.0520286560059}}, \"EndTime\": 1572370446.011808, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572370445.491998}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Total Batches Seen\": {\"count\": 1, \"max\": 729, \"sum\": 729.0, \"min\": 729}, \"Total Records Seen\": {\"count\": 1, \"max\": 725560, \"sum\": 725560.0, \"min\": 725560}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Reset Count\": {\"count\": 1, \"max\": 9, \"sum\": 9.0, \"min\": 9}}, \"EndTime\": 1572370446.012021, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 7}, \"StartTime\": 1572370445.492727}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #throughput_metric: host=algo-1, train throughput=174373.677504 records/second\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #quality_metric: host=algo-1, epoch=8, batch=0 train binary_classification_accuracy <score>=0.632\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #quality_metric: host=algo-1, epoch=8, batch=0 train binary_classification_cross_entropy <loss>=0.63976751709\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #quality_metric: host=algo-1, epoch=8, batch=0 train binary_f_1.000 <score>=0.758846657929\u001b[0m\n",
      "\u001b[31m[2019-10-29 17:34:06.544] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 530, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #quality_metric: host=algo-1, epoch=8, train binary_classification_accuracy <score>=0.623164835165\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #quality_metric: host=algo-1, epoch=8, train binary_classification_cross_entropy <loss>=0.657387503488\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #quality_metric: host=algo-1, epoch=8, train binary_f_1.000 <score>=0.720826481267\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 532.7181816101074, \"sum\": 532.7181816101074, \"min\": 532.7181816101074}}, \"EndTime\": 1572370446.545394, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572370446.011887}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Total Batches Seen\": {\"count\": 1, \"max\": 820, \"sum\": 820.0, \"min\": 820}, \"Total Records Seen\": {\"count\": 1, \"max\": 816130, \"sum\": 816130.0, \"min\": 816130}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1572370446.545645, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 8}, \"StartTime\": 1572370446.012644}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #throughput_metric: host=algo-1, train throughput=169880.65332 records/second\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #quality_metric: host=algo-1, epoch=9, batch=0 train binary_classification_accuracy <score>=0.639\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #quality_metric: host=algo-1, epoch=9, batch=0 train binary_classification_cross_entropy <loss>=0.637053527832\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:06 INFO 139641366697792] #quality_metric: host=algo-1, epoch=9, batch=0 train binary_f_1.000 <score>=0.761085373925\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-10-29 17:34:15 Uploading - Uploading generated training model\n",
      "2019-10-29 17:34:15 Completed - Training job completed\n",
      "\u001b[31m[2019-10-29 17:34:07.071] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 523, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] #quality_metric: host=algo-1, epoch=9, train binary_classification_accuracy <score>=0.630043956044\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] #quality_metric: host=algo-1, epoch=9, train binary_classification_cross_entropy <loss>=0.655000303834\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] #quality_metric: host=algo-1, epoch=9, train binary_f_1.000 <score>=0.723496172673\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] #quality_metric: host=algo-1, train binary_classification_accuracy <score>=0.630043956044\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] #quality_metric: host=algo-1, train binary_classification_cross_entropy <loss>=0.655000303834\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] #quality_metric: host=algo-1, train binary_f_1.000 <score>=0.723496172673\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 525.8579254150391, \"sum\": 525.8579254150391, \"min\": 525.8579254150391}}, \"EndTime\": 1572370447.072142, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572370446.545473}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Total Batches Seen\": {\"count\": 1, \"max\": 911, \"sum\": 911.0, \"min\": 911}, \"Total Records Seen\": {\"count\": 1, \"max\": 906700, \"sum\": 906700.0, \"min\": 906700}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90570, \"sum\": 90570.0, \"min\": 90570}, \"Reset Count\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}}, \"EndTime\": 1572370447.072343, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 9}, \"StartTime\": 1572370446.546257}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] #throughput_metric: host=algo-1, train throughput=172123.109903 records/second\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 WARNING 139641366697792] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 2.2430419921875, \"sum\": 2.2430419921875, \"min\": 2.2430419921875}}, \"EndTime\": 1572370447.074868, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572370447.072208}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] Saved checkpoint to \"/tmp/tmpl1Aj0U/state-0001.params\"\u001b[0m\n",
      "\u001b[31m[2019-10-29 17:34:07.081] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 5954, \"num_examples\": 1, \"num_bytes\": 64000}\u001b[0m\n",
      "\u001b[31m[2019-10-29 17:34:07.118] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 36, \"num_examples\": 10, \"num_bytes\": 603520}\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 9430, \"sum\": 9430.0, \"min\": 9430}, \"Total Batches Seen\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Total Records Seen\": {\"count\": 1, \"max\": 9430, \"sum\": 9430.0, \"min\": 9430}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 9430, \"sum\": 9430.0, \"min\": 9430}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1572370447.118361, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572370447.081731}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] #test_score (algo-1) : ('binary_classification_accuracy', 0.632661717921527)\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] #test_score (algo-1) : ('binary_classification_cross_entropy', 0.650316421654419)\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] #test_score (algo-1) : ('binary_f_1.000', 0.7510063254744106)\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] #quality_metric: host=algo-1, test binary_classification_accuracy <score>=0.632661717922\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] #quality_metric: host=algo-1, test binary_classification_cross_entropy <loss>=0.650316421654\u001b[0m\n",
      "\u001b[31m[10/29/2019 17:34:07 INFO 139641366697792] #quality_metric: host=algo-1, test binary_f_1.000 <score>=0.751006325474\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 6040.377855300903, \"sum\": 6040.377855300903, \"min\": 6040.377855300903}, \"setuptime\": {\"count\": 1, \"max\": 39.47091102600098, \"sum\": 39.47091102600098, \"min\": 39.47091102600098}}, \"EndTime\": 1572370447.119195, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572370447.074925}\n",
      "\u001b[0m\n",
      "Training seconds: 61\n",
      "Billable seconds: 61\n"
     ]
    }
   ],
   "source": [
    "fm = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n",
    "                                   get_execution_role(), \n",
    "                                   train_instance_count=1, \n",
    "                                   train_instance_type='ml.c4.xlarge',\n",
    "                                   output_path=output_prefix,\n",
    "                                   sagemaker_session=sagemaker.Session())\n",
    "\n",
    "fm.set_hyperparameters(feature_dim=nbFeatures,\n",
    "                      predictor_type='binary_classifier',\n",
    "                      mini_batch_size=1000,\n",
    "                      num_factors=64,\n",
    "                      epochs=10)\n",
    "\n",
    "fm.fit({'train': train_data, 'test': test_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***If the training was successful, you wil see 'Training job completed' at the end of the output. Scroll up to see the  train and test accuracy***    \n",
    "\n",
    "***After training phase completed, we have the model parameters stored in S3 (in the output path you specified). You can check your S3 bucket that contains the output to inspect how the training job output looks like***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM7: Deploy model\n",
    "\n",
    "***Now, let's deploy the model for inference using SageMaker SDK. It will spin-up a new virtual machine with container containing algorithm for inference. It will give us an API endpoint for inference.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "fm_predictor = fm.deploy(instance_type='ml.t2.medium', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM8: Run predictions\n",
    "\n",
    "***After the model is deployed and given an endpoint, we can run the prediction / inference.***  \n",
    "Below we define the serializer and deserializer for the prediction request/response data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fm_serializer(data):\n",
    "    js = {'instances': []}\n",
    "    for row in data:\n",
    "        js['instances'].append({'features': row.tolist()})\n",
    "    #print js\n",
    "    return json.dumps(js)\n",
    "\n",
    "fm_predictor.content_type = 'application/json'\n",
    "fm_predictor.serializer = fm_serializer\n",
    "fm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Let's test the prediction with some data from the test set***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_from = 900\n",
    "index_to = 910\n",
    "result = fm_predictor.predict(X_test[index_from:index_to].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Display the prediction in pretty table, being compared againts the actual rating (label) from the test set.***.      \n",
    "Observe that for score between 0.3 to 0.7 our recommender may guess incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Using cached https://files.pythonhosted.org/packages/66/d4/977fdd5186b7cdbb7c43a7aac7c5e4e0337a84cb802e154616f3cfc84563/tabulate-0.8.5.tar.gz\n",
      "Building wheels for collected packages: tabulate\n",
      "  Running setup.py bdist_wheel for tabulate ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/e1/41/5e/e201f95d90fc84f93aa629b6638adacda680fe63aac47174ab\n",
      "Successfully built tabulate\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.8.5\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Score           </td><td style=\"text-align: right;\">0.63</td><td style=\"text-align: right;\">0.66</td><td style=\"text-align: right;\">0.61</td><td style=\"text-align: right;\">0.49</td><td style=\"text-align: right;\">0.49</td><td style=\"text-align: right;\">0.61</td><td style=\"text-align: right;\">0.66</td><td style=\"text-align: right;\">0.65</td><td style=\"text-align: right;\">0.59</td><td style=\"text-align: right;\">0.51</td></tr>\n",
       "<tr><td>Predicted Rating</td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">0   </td><td style=\"text-align: right;\">0   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td></tr>\n",
       "<tr><td>Actual Rating   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">0   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">0   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">0   </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install tabulate\n",
    "import tabulate\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "scores, predicted_rating = ['Score'], ['Predicted Rating']\n",
    "for r in result['predictions']:\n",
    "    scores.append(\"%.2f\" % r['score'])\n",
    "    predicted_rating.append(r['predicted_label'])\n",
    "\n",
    "\n",
    "table = [scores, predicted_rating, ['Actual Rating'] + Y_test[index_from:index_to].tolist() ]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM9: Get Movies Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***After testing the prediction, let's get real movies recommendation for a particular user***    \n",
    "First, let's prepare a dictionary that maps movie ID to its title. We use the u.item data containing movies' details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = {}\n",
    "with open('./ml-100k/u.item','r') as f:\n",
    "    samples=csv.reader(f,delimiter='|')\n",
    "    for movieId,m_title,r_date,video_r_date,imdb_URL,unkwn,act,adv,anm,kid,cmd,crime,doc,drama,fantasy,f_noir,horror,msc,myst,rom,sfy,thriller,war,west in samples:\n",
    "        movies[int(movieId)] = m_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Define some parameters***    \n",
    "userId = the ID of user who needs the recommendations    \n",
    "score_threshold = Cut-off score. Value nearer to 1 means that we only consider strong predictions. Value 0.5 is the minimun.    \n",
    "maximum_recommendations = Maximum of movies recommendation. The actual result may be less than this if not many movies are strongly recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "userId = 344\n",
    "score_threshold = 0.50\n",
    "maximum_recommendations = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Run predictions for all movies for this particular user and sort the output based on score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_movies=[]\n",
    "for movieId in range(nbMovies):\n",
    "    test_input = lil_matrix((1, nbFeatures)).astype('float32')\n",
    "    test_input[0, int(userId)-1] = 1\n",
    "    test_input[0, nbUsers+int(movieId)-1] = 1\n",
    "    result = fm_predictor.predict(test_input.toarray())\n",
    "    result_label, result_score = int(result['predictions'][0]['predicted_label']), float(result['predictions'][0]['score'])\n",
    "    if (result_label == 1) and (result_score > score_threshold):\n",
    "        recommended_movies.append([int(movieId),result_score])\n",
    "        \n",
    "def getVal(item):\n",
    "    return item[1]\n",
    "recommended_movies = sorted(recommended_movies,key=getVal,reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Print out the result of top recommended movies***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td><strong>Movie Title</strong>          </td><td><strong>Score</strong></td></tr>\n",
       "<tr><td>Star Wars (1977)                      </td><td>0.698259413242        </td></tr>\n",
       "<tr><td>Silence of the Lambs, The (1991)      </td><td>0.685581743717        </td></tr>\n",
       "<tr><td>Schindler's List (1993)               </td><td>0.679210960865        </td></tr>\n",
       "<tr><td>Godfather, The (1972)                 </td><td>0.6779692173          </td></tr>\n",
       "<tr><td>Raiders of the Lost Ark (1981)        </td><td>0.677012860775        </td></tr>\n",
       "<tr><td>Shawshank Redemption, The (1994)      </td><td>0.674218654633        </td></tr>\n",
       "<tr><td>Casablanca (1942)                     </td><td>0.672584354877        </td></tr>\n",
       "<tr><td>One Flew Over the Cuckoo's Nest (1975)</td><td>0.669767141342        </td></tr>\n",
       "<tr><td>Usual Suspects, The (1995)            </td><td>0.668714165688        </td></tr>\n",
       "<tr><td>Fargo (1996)                          </td><td>0.667235612869        </td></tr>\n",
       "<tr><td>To Kill a Mockingbird (1962)          </td><td>0.665493249893        </td></tr>\n",
       "<tr><td>Rear Window (1954)                    </td><td>0.661836802959        </td></tr>\n",
       "<tr><td>Vertigo (1958)                        </td><td>0.6611956954          </td></tr>\n",
       "<tr><td>Titanic (1997)                        </td><td>0.661050856113        </td></tr>\n",
       "<tr><td>Empire Strikes Back, The (1980)       </td><td>0.66088283062         </td></tr>\n",
       "<tr><td>Fugitive, The (1993)                  </td><td>0.656806588173        </td></tr>\n",
       "<tr><td>Return of the Jedi (1983)             </td><td>0.655569970608        </td></tr>\n",
       "<tr><td>L.A. Confidential (1997)              </td><td>0.653602004051        </td></tr>\n",
       "<tr><td>Amadeus (1984)                        </td><td>0.652572751045        </td></tr>\n",
       "<tr><td>Blade Runner (1982)                   </td><td>0.652530193329        </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_table = [['<strong>Movie Title</strong>','<strong>Score</strong>']]\n",
    "for i in range(min(maximum_recommendations,len(recommended_movies))):\n",
    "    output_table.append([movies[int(recommended_movies[i][0])],recommended_movies[i][1]])\n",
    "\n",
    "display(HTML(tabulate.tabulate(output_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Compare the recommendation with the top 20 movies that are actually rated by that particular user, sorted from the highest rating***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td><strong>Movie Title</strong>                </td><td><strong>Actual Rating</strong></td></tr>\n",
       "<tr><td>Babe (1995)                                 </td><td>5                             </td></tr>\n",
       "<tr><td>Dead Man Walking (1995)                     </td><td>5                             </td></tr>\n",
       "<tr><td>Usual Suspects, The (1995)                  </td><td>5                             </td></tr>\n",
       "<tr><td>Postino, Il (1994)                          </td><td>5                             </td></tr>\n",
       "<tr><td>Eat Drink Man Woman (1994)                  </td><td>5                             </td></tr>\n",
       "<tr><td>Star Wars (1977)                            </td><td>5                             </td></tr>\n",
       "<tr><td>Shawshank Redemption, The (1994)            </td><td>5                             </td></tr>\n",
       "<tr><td>Blade Runner (1982)                         </td><td>5                             </td></tr>\n",
       "<tr><td>Fargo (1996)                                </td><td>5                             </td></tr>\n",
       "<tr><td>Lone Star (1996)                            </td><td>5                             </td></tr>\n",
       "<tr><td>Godfather, The (1972)                       </td><td>5                             </td></tr>\n",
       "<tr><td>Big Night (1996)                            </td><td>5                             </td></tr>\n",
       "<tr><td>Willy Wonka and the Chocolate Factory (1971)</td><td>5                             </td></tr>\n",
       "<tr><td>Wrong Trousers, The (1993)                  </td><td>5                             </td></tr>\n",
       "<tr><td>Princess Bride, The (1987)                  </td><td>5                             </td></tr>\n",
       "<tr><td>Raiders of the Lost Ark (1981)              </td><td>5                             </td></tr>\n",
       "<tr><td>Brazil (1985)                               </td><td>5                             </td></tr>\n",
       "<tr><td>Aliens (1986)                               </td><td>5                             </td></tr>\n",
       "<tr><td>Alien (1979)                                </td><td>5                             </td></tr>\n",
       "<tr><td>Henry V (1989)                              </td><td>5                             </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def find_top_rated_movies(user_id, k):\n",
    "    rated_movies = moviesByUser[str(int(user_id)-1)]\n",
    "    rated_movies = sorted(rated_movies,key=getVal,reverse=True)\n",
    "    results = []\n",
    "    \n",
    "    for movie in rated_movies:\n",
    "        results.append([movies[int(movie[0]+1)],movie[1]])\n",
    "    return results[0:k]\n",
    "\n",
    "output_table = [['<strong>Movie Title</strong>','<strong>Actual Rating</strong>']]\n",
    "for m in find_top_rated_movies(userId,20):\n",
    "    output_table.append(m)\n",
    "\n",
    "display(HTML(tabulate.tabulate(output_table, tablefmt='html')))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python2",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
