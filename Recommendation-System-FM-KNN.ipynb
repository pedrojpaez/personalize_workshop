{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook outlines how to build a recommendation system using SageMaker's Factorization Machines (FM). The main goal is to showcase how to extend FM model to predict top \"X\" recommendations using SageMaker's KNN and Batch Transform.\n",
    "\n",
    "There are four parts to this notebook:\n",
    "\n",
    "1. Building a FM Model\n",
    "2. Repackaging FM Model to fit a KNN Model\n",
    "3. Building a KNN model\n",
    "4. Running Batch Transform for predicting top \"X\" items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Building a FM Model using movie lens dataset\n",
    "\n",
    "Julien Simon has written a fantastic blog about how to build a FM model using SageMaker with detailed explanation. Please see the links below for more information. In this part, I utilized his code for the most part to have continutity for performing additional steps.\n",
    "\n",
    "Source - https://aws.amazon.com/blogs/machine-learning/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import json_deserializer\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "import pandas as pd\n",
    "import boto3, io, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download movie rating data from movie lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data\n",
    "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip -o ml-100k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf ml-100k/ua.base -o ml-100k/ua.base.shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_ratings_train = pd.read_csv('ml-100k/ua.base.shuffled', sep='\\t', index_col=False, \n",
    "                 names=['user_id' , 'movie_id' , 'rating'])\n",
    "user_movie_ratings_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_ratings_test = pd.read_csv('ml-100k/ua.test', sep='\\t', index_col=False, \n",
    "                 names=['user_id' , 'movie_id' , 'rating'])\n",
    "user_movie_ratings_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_users= user_movie_ratings_train['user_id'].max()\n",
    "nb_movies=user_movie_ratings_train['movie_id'].max()\n",
    "nb_features=nb_users+nb_movies\n",
    "nb_ratings_test=len(user_movie_ratings_test.index)\n",
    "nb_ratings_train=len(user_movie_ratings_train.index)\n",
    "print \" # of users: \", nb_users\n",
    "print \" # of movies: \", nb_movies\n",
    "print \" Training Count: \", nb_ratings_train\n",
    "print \" Test Count: \", nb_ratings_test\n",
    "print \" Features (# of users + # of movies): \", nb_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FM Input\n",
    "\n",
    "Input to FM is a one-hot encoded sparse matrix. Only ratings 4 and above are considered for the model. We will be ignoring ratings 3 and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(df, lines, columns):\n",
    "    # Features are one-hot encoded in a sparse matrix\n",
    "    X = lil_matrix((lines, columns)).astype('float32')\n",
    "    # Labels are stored in a vector\n",
    "    Y = []\n",
    "    line=0\n",
    "    for index, row in df.iterrows():\n",
    "            X[line,row['user_id']-1] = 1\n",
    "            X[line, nb_users+(row['movie_id']-1)] = 1\n",
    "            if int(row['rating']) >= 4:\n",
    "                Y.append(1)\n",
    "            else:\n",
    "                Y.append(0)\n",
    "            line=line+1\n",
    "\n",
    "    Y=np.array(Y).astype('float32')            \n",
    "    return X,Y\n",
    "\n",
    "\n",
    "X_train, Y_train = loadDataset(user_movie_ratings_train, nb_ratings_train, nb_features)\n",
    "X_test, Y_test = loadDataset(user_movie_ratings_test, nb_ratings_test, nb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "assert X_train.shape == (nb_ratings_train, nb_features)\n",
    "assert Y_train.shape == (nb_ratings_train, )\n",
    "zero_labels = np.count_nonzero(Y_train)\n",
    "print(\"Training labels: %d zeros, %d ones\" % (zero_labels, nb_ratings_train-zero_labels))\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "assert X_test.shape  == (nb_ratings_test, nb_features)\n",
    "assert Y_test.shape  == (nb_ratings_test, )\n",
    "zero_labels = np.count_nonzero(Y_test)\n",
    "print(\"Test labels: %d zeros, %d ones\" % (zero_labels, nb_ratings_test-zero_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Protobuf format for saving to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this value to your own bucket name\n",
    "bucket = ''\n",
    "prefix = 'fm'\n",
    "\n",
    "if bucket.strip() == '':\n",
    "    raise RuntimeError(\"bucket name is empty.\")\n",
    "\n",
    "train_key      = 'train.protobuf'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "\n",
    "test_key       = 'test.protobuf'\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test')\n",
    "\n",
    "output_prefix  = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeDatasetToProtobuf(X, bucket, prefix, key, d_type, Y=None):\n",
    "    buf = io.BytesIO()\n",
    "    if d_type == \"sparse\":\n",
    "        smac.write_spmatrix_to_sparse_tensor(buf, X, labels=Y)\n",
    "    else:\n",
    "        smac.write_numpy_to_dense_tensor(buf, X, labels=Y)\n",
    "        \n",
    "    buf.seek(0)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)\n",
    "    \n",
    "fm_train_data_path = writeDatasetToProtobuf(X_train, bucket, train_prefix, train_key, \"sparse\", Y_train)    \n",
    "fm_test_data_path  = writeDatasetToProtobuf(X_test, bucket, test_prefix, test_key, \"sparse\", Y_test)    \n",
    "  \n",
    "print \"Training data S3 path: \",fm_train_data_path\n",
    "print \"Test data S3 path: \",fm_test_data_path\n",
    "print \"FM model output S3 path: {}\".format(output_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training job\n",
    "\n",
    "You can play around with the hyper parameters until you are happy with the prediction. For this dataset and hyper parameters configuration, after 100 epochs, test accuracy was around 70% on average and the F1 score (a typical metric for a binary classifier) was around 0.74 (1 indicates a perfect classifier). Not great, but you can fine tune the model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type='ml.m5.large'\n",
    "fm = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"factorization-machines\"),\n",
    "                                   get_execution_role(), \n",
    "                                   train_instance_count=1, \n",
    "                                   train_instance_type=instance_type,\n",
    "                                   output_path=output_prefix,\n",
    "                                   sagemaker_session=sagemaker.Session())\n",
    "\n",
    "fm.set_hyperparameters(feature_dim=nb_features,\n",
    "                      predictor_type='binary_classifier',\n",
    "                      mini_batch_size=1000,\n",
    "                      num_factors=64,\n",
    "                      epochs=100)\n",
    "\n",
    "fm.fit({'train': fm_train_data_path, 'test': fm_test_data_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Repackaging Model data to fit a KNN Model\n",
    "\n",
    "Now that we have the model created and stored in SageMaker, we can download the same and repackage it to fit a KNN model. Note - install mxnet by uncommenting the first line below, if need be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mxnet\n",
    "import mxnet as mx\n",
    "model_file_name = \"model.tar.gz\"\n",
    "model_full_path = fm.output_path +\"/\"+ fm.latest_training_job.job_name +\"/output/\"+model_file_name\n",
    "print \"Model Path: \", model_full_path\n",
    "\n",
    "#Download FM model \n",
    "os.system(\"aws s3 cp \"+model_full_path+ \" .\")\n",
    "\n",
    "#Extract model file for loading to MXNet\n",
    "os.system(\"tar xzvf \"+model_file_name)\n",
    "os.system(\"unzip -o model_algo-1\")\n",
    "os.system(\"mv symbol.json model-symbol.json\")\n",
    "os.system(\"mv params model-0000.params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract model data to create item and user latent matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract model data\n",
    "m = mx.module.Module.load('./model', 0, False, label_names=['out_label'])\n",
    "V = m._arg_params['v'].asnumpy()\n",
    "w = m._arg_params['w1_weight'].asnumpy()\n",
    "b = m._arg_params['w0_weight'].asnumpy()\n",
    "\n",
    "# item latent matrix - concat(V[i], w[i]).  \n",
    "knn_item_matrix = np.concatenate((V[nb_users:], w[nb_users:]), axis=1)\n",
    "knn_train_label = np.arange(1,nb_movies+1)\n",
    "\n",
    "#user latent matrix - concat (V[u], 1) \n",
    "ones = np.ones(nb_users).reshape((nb_users, 1))\n",
    "knn_user_matrix = np.concatenate((V[:nb_users], ones), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Building KNN Model\n",
    "\n",
    "In this section, we upload the model input data to S3, create a KNN model and save the same. Saving the model, will display the model in the model section of SageMaker. Also, it will aid in calling batch transform down the line or even deploying it as an end point for real-time inference.\n",
    "\n",
    "This approach uses the default 'index_type' parameter for knn. It is precise but can be slow for large datasets. In such cases, you may want to use a different 'index_type' parameter leading to an approximate, yet fast answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('KNN train features shape = ', knn_item_matrix.shape)\n",
    "knn_prefix = 'knn'\n",
    "knn_output_prefix  = 's3://{}/{}/output'.format(bucket, knn_prefix)\n",
    "knn_train_data_path = writeDatasetToProtobuf(knn_item_matrix, bucket, knn_prefix, train_key, \"dense\", knn_train_label)\n",
    "print('uploaded KNN train data: {}'.format(knn_train_data_path))\n",
    "\n",
    "nb_recommendations = 100\n",
    "\n",
    "# set up the estimator\n",
    "knn = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"knn\"),\n",
    "    get_execution_role(),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=instance_type,\n",
    "    output_path=knn_output_prefix,\n",
    "    sagemaker_session=sagemaker.Session())\n",
    "\n",
    "knn.set_hyperparameters(feature_dim=knn_item_matrix.shape[1], k=nb_recommendations, index_metric=\"INNER_PRODUCT\", predictor_type='classifier', sample_size=200000)\n",
    "fit_input = {'train': knn_train_data_path}\n",
    "knn.fit(fit_input)\n",
    "knn_model_name =  knn.latest_training_job.job_name\n",
    "print \"created model: \", knn_model_name\n",
    "\n",
    "# save the model so that we can reference it in the next step during batch inference\n",
    "sm = boto3.client(service_name='sagemaker')\n",
    "primary_container = {\n",
    "    'Image': knn.image_name,\n",
    "    'ModelDataUrl': knn.model_data,\n",
    "}\n",
    "\n",
    "knn_model = sm.create_model(\n",
    "        ModelName = knn.latest_training_job.job_name,\n",
    "        ExecutionRoleArn = knn.role,\n",
    "        PrimaryContainer = primary_container)\n",
    "print \"saved the model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Batch Transform\n",
    "\n",
    "In this section, we will use SageMaker's batch transform option to batch predict top X for all the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#upload inference data to S3\n",
    "knn_batch_data_path = writeDatasetToProtobuf(knn_user_matrix, bucket, knn_prefix, train_key, \"dense\")\n",
    "print \"Batch inference data path: \",knn_batch_data_path\n",
    "\n",
    "# Initialize the transformer object\n",
    "transformer =sagemaker.transformer.Transformer(\n",
    "    base_transform_job_name=\"knn\",\n",
    "    model_name=knn_model_name,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    output_path=knn_output_prefix,\n",
    "    accept=\"application/jsonlines; verbose=true\"\n",
    ")\n",
    "\n",
    "# Start a transform job:\n",
    "transformer.transform(knn_batch_data_path, content_type='application/x-recordio-protobuf')\n",
    "transformer.wait()\n",
    "\n",
    "\n",
    "#Download predictions \n",
    "results_file_name = \"inference_output\"\n",
    "inference_output_file = \"knn/output/train.protobuf.out\"\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.download_file(bucket, inference_output_file, results_file_name)\n",
    "with open(results_file_name) as f:\n",
    "    results = f.readlines()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "test_user_idx = 89\n",
    "u_one_json = json.loads(results[test_user_idx])\n",
    "\n",
    "print \"Recommended movie Ids for user #{} : {}\".format(test_user_idx+1, [int(movie_id) for movie_id in u_one_json['labels']])\n",
    "print\n",
    "print \"Movie distances for user #{} : {}\".format(test_user_idx+1,  [round(distance, 4) for distance in u_one_json['distances']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p27",
   "language": "python",
   "name": "conda_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
